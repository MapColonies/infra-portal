{"searchDocs":[{"title":"Guides","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/guides","content":"Guides","keywords":"","version":"Next"},{"title":"Helm Diff","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/guides/DevOps/helm-diff","content":"","keywords":"","version":"Next"},{"title":"How to installâ€‹","type":1,"pageTitle":"Helm Diff","url":"/infra-portal/pr-preview/pr-26/docs/guides/DevOps/helm-diff#how-to-install","content":" ","version":"Next","tagName":"h2"},{"title":"On a computer with internet accessâ€‹","type":1,"pageTitle":"Helm Diff","url":"/infra-portal/pr-preview/pr-26/docs/guides/DevOps/helm-diff#on-a-computer-with-internet-access","content":" helm plugin install https://github.com/databus23/helm-diff  ","version":"Next","tagName":"h3"},{"title":"On a computer without internet accessâ€‹","type":1,"pageTitle":"Helm Diff","url":"/infra-portal/pr-preview/pr-26/docs/guides/DevOps/helm-diff#on-a-computer-without-internet-access","content":" Assuming that you're working on your Windows workstation in the Outnet you can install the plugin by copying a few files into your HELM_PLUGINS directory.  Go to the helm-diff gitlab repo.Clone the helm-diff gitlab repo to your workstation.In the root directory you'll find a directory called tar-gz, cd into this directory.Extract the helm-diff-windows-amd64.tgzFind out what's the HELM_PLUGINS path configured on your computer by running helm env, in the out output you'll find all the helm env vars, copy the path.If the HELM_PLUGINS doesn't exist create it.Create a directory helm-diff under the HELM_PLUGINS path.From the extracted tgz file, copy the bin directory and the plugin.yaml into the HELM_PLUGINS/helm-diff directory.  Now you have helm-diff plugin installed on your Windows workstation.  ","version":"Next","tagName":"h3"},{"title":"How to use itâ€‹","type":1,"pageTitle":"Helm Diff","url":"/infra-portal/pr-preview/pr-26/docs/guides/DevOps/helm-diff#how-to-use-it","content":" Assuming that your chart name is foo and your values file name is dev-values.yamlrun the following from the Chart directory you wish to upgrade. helm diff upgrade foo --values dev-values.yaml . You can also specify multiple values files: helm diff upgrade foo --values dev-values.yaml --values dev1-values.yaml . If there's a difference, the output will show all the diffs between the k8s objects which will be modified/create/deleted.  Here's an example of the helm diff output ","version":"Next","tagName":"h2"},{"title":"Knowledge Base","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base","content":"Knowledge Base","keywords":"","version":"Next"},{"title":"Pipelines","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/guides/DevOps/pipelines","content":"","keywords":"","version":"Next"},{"title":"Bundler/Unbundlerâ€‹","type":1,"pageTitle":"Pipelines","url":"/infra-portal/pr-preview/pr-26/docs/guides/DevOps/pipelines#bundlerunbundler","content":"   ","version":"Next","tagName":"h2"},{"title":"Bundler pipelineâ€‹","type":1,"pageTitle":"Pipelines","url":"/infra-portal/pr-preview/pr-26/docs/guides/DevOps/pipelines#bundler-pipeline","content":" The bundler pipeline has 1 parameter which is the artifacts.json file.  Here's an example of the artifacts.json  { &quot;docker&quot;: { &quot;acrarolibotnonprod.azurecr.io&quot;: { &quot;3d-gateway&quot;: &quot;v2.4.2&quot;, &quot;store-trigger&quot;: &quot;v1.5.2&quot; }, &quot;docker.io&quot;: { &quot;bitnami/redis&quot;: &quot;7.2.3&quot;, &quot;timonwong/uwsgi-exporter&quot;: &quot;latest&quot; } }, &quot;helm&quot;: { &quot;acrarolibotnonprod.azurecr.io&quot;: { &quot;gateway&quot;: &quot;2.4.2&quot;, &quot;store-trigger&quot;: &quot;1.5.2&quot; }, &quot;https://charts.bitnami.com/bitnami&quot;: { &quot;redis&quot;: &quot;18.5.0&quot; } }, &quot;git&quot;: [ &quot;MapColonies/helm-charts&quot; ] }   In order to trigger the bundler pipeline you'll need to follow the steps below:  Go to the bundler pipelineClick on Build with parameters Click on Choose File and upload the artifacts.json from you local computer. Click the Build buttonAfter the bundler will finish it'll send a slack message to the &lt;team&gt;-notifications channel based on the user logged in.Click on the download link in the slack message and download the bundle zip file which you'll use later on in the unbundler in the relevant network.  ","version":"Next","tagName":"h3"},{"title":"Unbundler pipelineâ€‹","type":1,"pageTitle":"Pipelines","url":"/infra-portal/pr-preview/pr-26/docs/guides/DevOps/pipelines#unbundler-pipeline","content":" In order to trigger the unbundler, open the jenkins instance in the relevant network where you want to to unbundle and click on the unbundler pipeline and follow the steps bellow.  Click on Build with ParametersClick on Choose file and select the bundler zip file from your computerClick the build buttonOnce the bundler is done you'll be able to find all the artifacts which were in the bundle zip file in all the registries in the specific network you're working on. ","version":"Next","tagName":"h3"},{"title":"Config Management - What Is It?","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/config-management","content":"","keywords":"","version":"Next"},{"title":"Motivationâ€‹","type":1,"pageTitle":"Config Management - What Is It?","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/config-management#motivation","content":" The MapColoniesâ„¢ Config Management system provides a centralized solution for managing all service configurations. This system ensures a single source of truth, simplifying configuration management. It also validates configurations against a JSON Schema, guaranteeing their correctness. Additionally, schema validation ensures that all configurations are strongly typed, enabling seamless integration with your code.  This system will also helps you handle config changes easier. If any of the config has changed, instead of changing the env and redeploy all the deployment, you can just rollout the pod in order to get the new config.  Now you can see the zero to hero doc and integrate this system in your service!  ","version":"Next","tagName":"h2"},{"title":"What's Nextâ€‹","type":1,"pageTitle":"Config Management - What Is It?","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/config-management#whats-next","content":" Customizable Configuration for Testing: We'll add the ability to customize configurations specifically for testing purposes.Enhanced Observability: We'll add improved observability features to facilitate more detailed analysis.UI Improvements: We'll refine the user interface to enhance interaction and usability. ","version":"Next","tagName":"h2"},{"title":"References","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/config-management/references","content":"","keywords":"","version":"Next"},{"title":"How does it workâ€‹","type":1,"pageTitle":"References","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/config-management/references#how-does-it-work","content":" When you pass a reference to another config instance, the configuration manager will parse and save the references so it can be resolved later. When the configuration is resolved, the configuration manager will replace the reference with the actual configuration.  For the server to understand a reference it needs to include both the config name and the version of the config. It's also possible to set the version as latest to always get the latest version of the referenced config.  When the final configuration is resolved, the referenced config will be merged with the current configuration under the same hierarchy.  ","version":"Next","tagName":"h2"},{"title":"Reference structureâ€‹","type":1,"pageTitle":"References","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/config-management/references#reference-structure","content":" In order to use a reference, you need to pass an object with the following structure:  { &quot;$ref&quot;: { &quot;configName&quot;: &quot;requested-config&quot;, &quot;version&quot;: &quot;latest&quot; } }   tip In the config-ui config editor, you can press Ctrl + Space to insert a snippet with the reference structure.  ","version":"Next","tagName":"h2"},{"title":"Example usageâ€‹","type":1,"pageTitle":"References","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/config-management/references#example-usage","content":" Let's say we want to create a configuration for a database connection for our app. We know there is already a configuration for the database connection in the db-partial config that is only missing the database name.  db-partial { &quot;ssl&quot;: { &quot;enabled&quot;: false }, &quot;host&quot;: &quot;avi&quot; }   We can create a new configuration that references the db-partial config and only sets the database name.  db-full { &quot;$ref&quot;: { &quot;configName&quot;: &quot;db-partial&quot;, &quot;version&quot;: &quot;latest&quot; }, &quot;database&quot;: &quot;my-db&quot; }   The resolved configuration will be:  { &quot;ssl&quot;: { &quot;enabled&quot;: false }, &quot;host&quot;: &quot;avi&quot;, &quot;database&quot;: &quot;my-db&quot; }  ","version":"Next","tagName":"h2"},{"title":"CI","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/DevOps/pipelines/ci","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"CI","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/DevOps/pipelines/ci#overview","content":" The CI (Continues Integration) pipelines are responsible for validating, compiling and building the artifacts and publishing them to the various registries.  ","version":"Next","tagName":"h2"},{"title":"Backgroundâ€‹","type":1,"pageTitle":"CI","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/DevOps/pipelines/ci#background","content":" We store all our helm charts of all the teams under the helm-charts repository  This repository is a mono repo, where each team has itâ€™s own scope. For example, 3D team has itâ€™s own directory where the team is storing their helm charts.  Hereâ€™s the repo structure:  - / - 3D/ - Umbrella chart - deployment.json - artifacts.json - raster/ - Umbrella chart - deployment.json - artifacts.json   ","version":"Next","tagName":"h2"},{"title":"Release Workflowâ€‹","type":1,"pageTitle":"CI","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/DevOps/pipelines/ci#release-workflow","content":"   The release workflow begins with a new release or tag push on each service repository and is executed using Github Actions workflows.  The workflows are being triggered automatically. If we take a look at a sample repo store-trigger it has a workflow called Build and push artifacts , where the source of this can be found here  The workflow is building the docker image of the service, tagging it and pushing it to Azure ACR. Along with the docker image the flow also builds and pushes the helm package which is part of the repository. Afterwards it's updating the artifacts.json which is associated with the team's scope under the helm-charts repository with the new releases which have been just released. ","version":"Next","tagName":"h2"},{"title":"Bundler & Unbundler","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/DevOps/pipelines/bundler-unbundler","content":"","keywords":"","version":"Next"},{"title":"Bundler pipelineâ€‹","type":1,"pageTitle":"Bundler & Unbundler","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/DevOps/pipelines/bundler-unbundler#bundler-pipeline","content":" The bundler pipeline has 1 parameter which is the artifacts.json file.  Here's an example of the artifacts.json  { &quot;docker&quot;: { &quot;acrarolibotnonprod.azurecr.io&quot;: { &quot;3d-gateway&quot;: &quot;v2.4.2&quot;, &quot;store-trigger&quot;: &quot;v1.5.2&quot; }, &quot;docker.io&quot;: { &quot;bitnami/redis&quot;: &quot;7.2.3&quot;, &quot;timonwong/uwsgi-exporter&quot;: &quot;latest&quot; } }, &quot;helm&quot;: { &quot;acrarolibotnonprod.azurecr.io&quot;: { &quot;gateway&quot;: &quot;2.4.2&quot;, &quot;store-trigger&quot;: &quot;1.5.2&quot; }, &quot;https://charts.bitnami.com/bitnami&quot;: { &quot;redis&quot;: &quot;18.5.0&quot; } }, &quot;git&quot;: [ &quot;MapColonies/helm-charts&quot; ] }   The bundler pipeline will bundle all the artifacts (docker tar balls, helm packages, git repos) into 1 zip file.  Once the bundler is done it'll send a slack message to the &lt;team&gt;-notifications channel based on the user logged in.    ","version":"Next","tagName":"h2"},{"title":"Unbundler pipelineâ€‹","type":1,"pageTitle":"Bundler & Unbundler","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/DevOps/pipelines/bundler-unbundler#unbundler-pipeline","content":" The unbundler pipeline is happening in other networks we have. After copying over the bundle file to the relevant network, there will be an unbundler pipeline on the jenkins.  The unbundler pipeline is performing the following:  For each docker tar ball it'll attempt to push the docker image with the same tag to all the docker registries available in all the sites.For each helm package it'll attempt to push the helm package with the same version and name to all the helm registries available in all the sites.For each git repository it'll attempt to open a PR/MR from the source master branch to the destination repository internally to the master branch. ","version":"Next","tagName":"h2"},{"title":"CD","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/DevOps/pipelines/cd","content":"","keywords":"","version":"Next"},{"title":"How to login?â€‹","type":1,"pageTitle":"CD","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/DevOps/pipelines/cd#how-to-login","content":" Each team has their own set of credentials which they can use to login to the jenkins instance.  We have 2 options for deployment:  Manual deploymentAuto Deploy  ","version":"Next","tagName":"h3"},{"title":"Auto deploymentâ€‹","type":1,"pageTitle":"CD","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/DevOps/pipelines/cd#auto-deployment","content":" The auto deployment process is currently monitoring the following branches: master and qa. On each commit to the helm-charts repository on the monitored branch the pipeline is kicking off.  The pipeline can be found here. As you can see in the overview page of the pipeline there are 2 branches which it's monitoring.    How the auto deploy pipeline is working?â€‹  The pipeline is first of all looking at changes introduced in each commit with a git diff.After finding the diff it'll look at the deployment.json in the scope (each team's directory in the helm-charts repo) directories.Here's an example of deployment.json file.  [ { &quot;chartLocation&quot;: &quot;3d/charts/ingestion&quot;, &quot;valuesFile&quot;: [&quot;3d/charts/global-values.yaml&quot;, &quot;3d/charts/ingestion/ingestion-values.yaml&quot;], &quot;namespace&quot;: &quot;3d-dev&quot;, &quot;releaseName&quot;: &quot;ingestion-3d&quot;, &quot;skip&quot;: false }, { &quot;chartLocation&quot;: &quot;3d/charts/serving&quot;, &quot;valuesFile&quot;: [&quot;3d/charts/global-values.yaml&quot;, &quot;3d/charts/serving/serving-values.yaml&quot;], &quot;namespace&quot;: &quot;3d-dev&quot;, &quot;releaseName&quot;: &quot;serving-3d&quot;, &quot;skip&quot;: true } ]   If you wish to skip a deployment you'll need to have &quot;skip&quot;: true in the chart you wish to skip.When the pipeline will start it'll send a slack message to the &lt;team&gt;-notifications channel based on chart location(s) which are going to be deployed. After the pipeline will finish it'll send another message to the &lt;team&gt;-notifications channel based on chart location(s) which have been deployed with the status of the deployment. ","version":"Next","tagName":"h3"},{"title":"Monitoring Stack Overview","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring","content":"Monitoring Stack Overview The monitoring stack is a Kubernetes-based stack used to observe, monitor, and alert on the health and performance metrics. It leverages open-source tools, including Prometheus, Grafana, Blackbox Exporter, Alertmanager, and OpenTelemetry.","keywords":"","version":"Next"},{"title":"Alertmanager","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/Alertmanager","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Alertmanager","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/Alertmanager#overview","content":" Alertmanager is responsible for handling and routing alerts generated by Prometheus. It manages alert notifications by grouping, routing, and sending alerts to channels such as Slack, email, or other messaging platforms. This helps reduce alert noise and ensures that the correct teams are notified.  The system supports alerts categorized by:  Severity: Critical, Warning, and Info levels.Environment: Alerts can be tagged with production, staging, or development to indicate the context.Service Labels: Alerts often include labels like service and team to provide clarity on the affected component.  tip You can read more about Alertmanager in the Alertmanager documentation  ","version":"Next","tagName":"h2"},{"title":"Home Screen Overviewâ€‹","type":1,"pageTitle":"Alertmanager","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/Alertmanager#home-screen-overview","content":" You can access Libot's Alertmanager at the MapColonies Alertmanager.  The Alertmanager home screen provides an overview of active alerts, alert groups, and their status. You can see how alerts are routed, which notifications are being sent, and to whom.    ","version":"Next","tagName":"h2"},{"title":"Alert Routingâ€‹","type":1,"pageTitle":"Alertmanager","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/Alertmanager#alert-routing","content":" Alertmanager enables you to define routes that determine where alerts should be sent, based on labels such as alert severity or environment. It supports routing alerts to various platforms, including Slack, classified chat systems, and email.  note The routing configuration is global and centrally managed by the infra team. Developers do not need to configure routing themselves. If you have specific requirements or changes, contact the infra team to discuss adjustments to the global configuration.  ","version":"Next","tagName":"h2"},{"title":"Managing Alertsâ€‹","type":1,"pageTitle":"Alertmanager","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/Alertmanager#managing-alerts","content":" Alerts can be managed in the Alertmanager UI, where you can silence, group, and view the history of all alerts. Here's an example of an active alert in the system:    ","version":"Next","tagName":"h2"},{"title":"Conclusionâ€‹","type":1,"pageTitle":"Alertmanager","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/Alertmanager#conclusion","content":" Alertmanager is crucial in ensuring the right people are notified in the right way, reducing noise and keeping alerts manageable. With routing and grouping options, you can finely tune how your team receives and handles critical issues. ","version":"Next","tagName":"h2"},{"title":"Grafana","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/Grafana","content":"","keywords":"","version":"Next"},{"title":"Multiple Data Sourcesâ€‹","type":1,"pageTitle":"Grafana","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/Grafana#multiple-data-sources","content":" Grafana supports connecting to multiple data sources, allowing you to visualize and analyze data from various platforms like Prometheus, Elasticsearch, and Tempo in to a single dashboard.  For a full list of supported data sources, check out the Grafana documentation on data sources. ","version":"Next","tagName":"h2"},{"title":"BlackBox Exporter","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/BlackBox","content":"","keywords":"","version":"Next"},{"title":"Scrape Setupâ€‹","type":1,"pageTitle":"BlackBox Exporter","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/BlackBox#scrape-setup","content":" To scrape your endpoint, you need to create an entry in this section of the prometheus.yml configuration:  Scrape Config extraScrapeConfigs: | - job_name: 'blackbox' metrics_path: /probe scrape_interval: 15s params: module: [http_2xx] # Look for an HTTP 200 response. static_configs: - targets: - https://your-endpoint.com - https://another-endpoint.com   Key Configuration Elements job_name: The name of the scrape job (blackbox in this example) used to identify the probe.metrics_path: Specifies the path for scraping metrics. The Blackbox Exporter's metrics are available at /probe.scrape_interval: Defines how often Prometheus will probe the endpoints. Here, it probes every 15 seconds.params: Parameters passed to the probe. For instance, module: [http_2xx] ensures the probe checks for a successful HTTP 200 response.static_configs: Lists the endpoints to probe.  To add, update, or remove an endpoint to scrape, or to modify an entire scrape job, you need to create a pull requesthere. After that, please notify the infra team for a review and upgrade.  ","version":"Next","tagName":"h2"},{"title":"Alertingâ€‹","type":1,"pageTitle":"BlackBox Exporter","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/BlackBox#alerting","content":" After setting up scraping with the Blackbox Exporter, you may want to configure alerts to monitor the health and performance of the scraped endpoints. For more details about setting up alerts with Prometheus, see the Prometheus Alerts section. ","version":"Next","tagName":"h2"},{"title":"Prometheus","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/Prometheus","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Prometheus","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/Prometheus#overview","content":" Prometheus is an open-source monitoring and alerting system designed to collect and store metrics as time series data. It scrapes metrics from targets like applications pods and servers, stores them with the timestamp and labels and allows querying using PromQL. Prometheus is commonly used to track system performance and set up alerts based on defined conditions.  tip You can read more about Prometheus in the Prometheus documentation  ","version":"Next","tagName":"h2"},{"title":"Home Screen Overviewâ€‹","type":1,"pageTitle":"Prometheus","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/Prometheus#home-screen-overview","content":" You can access Libot's Prometheus at https://prometheus.mapcolonies.net.  The Prometheus home screen provides an overview of the current status of the Prometheus server. Here, you can see the targets that are being monitored, the current configuration of Prometheus, and access the built-in query interface.    ","version":"Next","tagName":"h2"},{"title":"Prometheus Scrape Configurationâ€‹","type":1,"pageTitle":"Prometheus","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/Prometheus#prometheus-scrape-configuration","content":" Prometheus scrapes metrics from pods if they are in specific namespaces, properly annotated, and expose metrics on the specified port.  ","version":"Next","tagName":"h2"},{"title":"From Zero to Hero","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/guides/config-management/zero-to-hero","content":"","keywords":"","version":"Next"},{"title":"Assumptionsâ€‹","type":1,"pageTitle":"From Zero to Hero","url":"/infra-portal/pr-preview/pr-26/docs/guides/config-management/zero-to-hero#assumptions","content":" This guide assumes that you have an already existing service based on the MapColonies boilerplate repo, and basic knowledge of JSON Schema. If needed you can read and learn about JSON Schema in the following link: https://json-schema.org/understanding-json-schema.  It is also recommended to read the schemas package readme.  ","version":"Next","tagName":"h3"},{"title":"Before we startâ€‹","type":1,"pageTitle":"From Zero to Hero","url":"/infra-portal/pr-preview/pr-26/docs/guides/config-management/zero-to-hero#before-we-start","content":" In general, the implementation of the MapColoniesâ„¢ Config Management system consists of the following steps:  Work environment setup.Define a JSON Schema for your service configuration.Validating that the schema and the generated types are correct.Integrating the schema with your service. Below each step is explained in detail.  ","version":"Next","tagName":"h2"},{"title":"Initialize the work environmentâ€‹","type":1,"pageTitle":"From Zero to Hero","url":"/infra-portal/pr-preview/pr-26/docs/guides/config-management/zero-to-hero#initialize-the-work-environment","content":" Clone the schemas repo into your own machine:  git clone git@github.com:MapColonies/schemas.git   Change the working directory.  cd schemas   Install the dependencies.  npm install   Create a new branch for your schema.  git branch &lt;my-branch-name&gt;   Open the repository in your editor (vscode for this guide).  code .   Create a file for your schema under the schemas folder. The directory hierarchy represents the ID of the schema. For example, a common schema that handles redis configuration might be under schemas/common/redis/v1.schema.json and its id will be https://mapcolonies.com/common/redis/v1. The file name is based on the order of the schema. If it's the first one, the name should be v1.schema.json, otherwise it should be the next number in order.  ","version":"Next","tagName":"h2"},{"title":"Creating the schemaâ€‹","type":1,"pageTitle":"From Zero to Hero","url":"/infra-portal/pr-preview/pr-26/docs/guides/config-management/zero-to-hero#creating-the-schema","content":" Start by filling all the metadata for your JSON Schema. The more metadata you fill, it will be easier to understand and use your schema.  schemas/my-domain/my-schema/v1.schema.json { &quot;$id&quot;: &quot;https://mapcolonies.com/my-domain/my-schema/v1&quot;, &quot;type&quot;: &quot;object&quot;, &quot;title&quot;: &quot;myDomainMySchemaV1&quot;, &quot;description&quot;: &quot;My domain's schema&quot; }   Create your schema content. Use the official JSON Schema docs, The tips page and check other schemas in the repo for reference.  schemas/my-domain/my-schema/v1.schema.json { &quot;$id&quot;: &quot;https://mapcolonies.com/my-domain/my-schema/v1&quot;, &quot;type&quot;: &quot;object&quot;, &quot;title&quot;: &quot;myDomainMySchemaV1&quot;, &quot;description&quot;: &quot;My domain's schema&quot;, &quot;properties&quot;: { &quot;id&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;The unique identifier for the entity&quot; }, &quot;name&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;The name of the entity&quot; }, &quot;age&quot;: { &quot;type&quot;: &quot;integer&quot;, &quot;description&quot;: &quot;The age of the entity&quot;, &quot;x-env-value&quot;: &quot;ENTITY_AGE&quot; }, &quot;isAlive&quot;: { &quot;type&quot;: &quot;boolean&quot;, &quot;description&quot;: &quot;Is the entity alive&quot;, &quot;x-env-value&quot;: &quot;ENTITY_IS_ALIVE&quot; } } }   tip You can use x-env-value to enable overriding the value of a field using environment variable. For more information check out the relevant docs in config and schemas.  If we want our service schema to extend the base boilerplate schema we could do this like that:  { &quot;$id&quot;: &quot;https://mapcolonies.com/my-domain/my-schema/v1&quot;, &quot;type&quot;: &quot;object&quot;, &quot;title&quot;: &quot;myDomainMySchemaV1&quot;, &quot;description&quot;: &quot;My domain's schema&quot;, &quot;allOf&quot;: [ { &quot;$ref&quot;: &quot;https://mapcolonies.com/common/boilerplate/v4&quot; }, { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;myDomain&quot;: { &quot;$ref&quot;: &quot;#/definitions/myDomainMySchemaV1&quot; } } } ], &quot;definitions&quot;: { &quot;myDomainMySchemaV1&quot;: { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;id&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;The unique identifier for the entity&quot; }, &quot;name&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;The name of the entity&quot; }, &quot;age&quot;: { &quot;type&quot;: &quot;integer&quot;, &quot;description&quot;: &quot;The age of the entity&quot;, &quot;x-env-value&quot;: &quot;ENTITY_AGE&quot; }, &quot;isAlive&quot;: { &quot;type&quot;: &quot;boolean&quot;, &quot;description&quot;: &quot;Is the entity alive&quot;, &quot;x-env-value&quot;: &quot;ENTITY_IS_ALIVE&quot; } } } } }    The result is a combined object with both the boilerplate schema and our own properties.  important When extending the boilerplate, you should check and make sure you extend the newest version. The version in this guide might be older.  ","version":"Next","tagName":"h2"},{"title":"Run validationsâ€‹","type":1,"pageTitle":"From Zero to Hero","url":"/infra-portal/pr-preview/pr-26/docs/guides/config-management/zero-to-hero#run-validations","content":" We want to make sure that the schema are valid, therefore we need to check the schema for errors. To do so we can run the following command and make changes based on the feedback.  npm run validate   info To see all the validations check the following doc: https://github.com/MapColonies/schemas?tab=readme-ov-file#validations  ","version":"Next","tagName":"h2"},{"title":"Validating the typesâ€‹","type":1,"pageTitle":"From Zero to Hero","url":"/infra-portal/pr-preview/pr-26/docs/guides/config-management/zero-to-hero#validating-the-types","content":" The types that the schemas package exports are automatically generated from the json schema. We want to make sure that the types are correct before we move forward. This way we can check the types and if anything is wrong go back and fix the schema.  Build the package:  npm run build   Check that the types are as you expected. They can be found under the build directory under the same hierarchy as the schema. For example, those are the types generated for the schema we created above extending the boilerplate schema:  build/schemas/my-domain/my-schema/v1.schema.d.ts import { typeSymbol } from '../../symbol.js'; declare const exported: { readonly [typeSymbol]: { [x: string]: unknown; myDomain?: { [x: string]: unknown; id?: string | undefined; name?: string | undefined; age?: number | undefined; isAlive?: boolean | undefined; } | undefined; openapiConfig: { [x: string]: unknown; filePath: string; basePath: string; rawPath: string; uiPath: string; }; telemetry: { [x: string]: unknown; logger: { [x: string]: unknown; level: &quot;info&quot; | &quot;trace&quot; | &quot;debug&quot; | &quot;warn&quot; | &quot;error&quot; | &quot;fatal&quot;; prettyPrint: boolean; }; shared: { [x: string]: unknown; serviceName?: string | undefined; serviceVersion?: string | undefined; hostname?: string | undefined; }; tracing: { [x: string]: unknown; debug?: boolean | undefined; url?: string | undefined; traceRatio?: number | undefined; isEnabled: boolean; }; }; server: { [x: string]: unknown; port: number; request: { [x: string]: unknown; payload: { [x: string]: unknown; limit: string; }; }; }; }; ...   note The rest of the file is important for the inner working of the schemas package. You should check only the types.  ","version":"Next","tagName":"h2"},{"title":"Checking integration with your serviceâ€‹","type":1,"pageTitle":"From Zero to Hero","url":"/infra-portal/pr-preview/pr-26/docs/guides/config-management/zero-to-hero#checking-integration-with-your-service","content":" Before merging the changes you made, we want to make sure it works with your service.  install the development version of the schemas package into your service. There are multiple ways to achieve that, with some of them described below.  GithubLocal PathNPM PackOther Options When opening a PR in the schemas repo, the latest version of your branch will be built and be available for a week. Push your changes to the remote repository. git push --set-upstream origin &lt;my-branch-name&gt; Open a PR in the schemas repo.If all the validations pass, a comment will appear with instructions and link to the package.Insert the link into your service's package.json file. package.json { &quot;dependencies&quot;: { &quot;@map-colonies/schemas&quot;: &lt;insert-generated-link&gt; } } Install the package. npm install     Change the config in your service to use the new schema.  src/common/config.ts import { type ConfigInstance, config } from '@map-colonies/config'; import { &lt;your-new-schema&gt;, type [your-new-schema-type] } from '@map-colonies/schemas'; // Choose here the type of the config instance and import this type from the entire application type ConfigType = ConfigInstance&lt;[your-new-schema-type]&gt;; let configInstance: ConfigType | undefined; /** * Initializes the configuration by fetching it from the server. * This should only be called from the instrumentation file. * @returns A Promise that resolves when the configuration is successfully initialized. */ async function initConfig(offlineMode?:boolean): Promise&lt;void&gt; { configInstance = await config({ configName: 'boiler-config', configServerUrl: 'http://localhost:8080', schema: &lt;your-new-schema&gt;, version: 'latest', offlineMode: offlineMode }); }   Check that the service works as expected.  ","version":"Next","tagName":"h2"},{"title":"Next stepsâ€‹","type":1,"pageTitle":"From Zero to Hero","url":"/infra-portal/pr-preview/pr-26/docs/guides/config-management/zero-to-hero#next-steps","content":" Now that you finished writing your schema, you can submit a PR to the schemas repo and ask for it to be reviewed.  We hope this guide was helpful and you were able to integrate the MapColoniesâ„¢ Config Management system into your service.  If you have any questions or need help, feel free to ask for help.  If you have any suggestions for this guide, please open a pull request. ","version":"Next","tagName":"h2"},{"title":"OpenTelemetry Collector","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/OpenTelemetryCollector","content":"","keywords":"","version":"Next"},{"title":"Example: Collecting Tracesâ€‹","type":1,"pageTitle":"OpenTelemetry Collector","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/OpenTelemetryCollector#example-collecting-traces","content":" To collect distributed traces from your services and send them to Grafana Tempo for visualization, you need to configure the OpenTelemetry Collector to receive and export traces. Hereâ€™s an example configuration:  tracing: enabled: true url: http://infra-monitoring-opentelemetry-collector.infra-services.svc.cluster.local:4317   ","version":"Next","tagName":"h2"},{"title":"Sampling in OpenTelemetryâ€‹","type":1,"pageTitle":"OpenTelemetry Collector","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/OpenTelemetryCollector#sampling-in-opentelemetry","content":" Sampling controls which traces are collected and exported to manage data volume. Common strategies include: head sampling (deciding at the start of a trace) and tail sampling (deciding after the trace completes). This helps balance observability with resource efficiency. ","version":"Next","tagName":"h2"},{"title":"Configuring Your Service for Prometheus Scrapingâ€‹","type":1,"pageTitle":"Prometheus","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/Prometheus#configuring-your-service-for-prometheus-scraping","content":" To ensure your pod is scraped, it should be deployed in one of the listed namespaces and annotated correctly. The required annotation looks like this:   podAnnotations: prometheus.io/scrape: &quot;true&quot; prometheus.io/port: &quot;10000&quot;   ","version":"Next","tagName":"h3"},{"title":"Prometheus Alertsâ€‹","type":1,"pageTitle":"Prometheus","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/Prometheus#prometheus-alerts","content":" tip You can access the official Prometheus alerting documentation  Prometheus Alerts are rules defined in Prometheus that automatically trigger notifications when certain conditions are met, such as high CPU usage or service downtime.    These alert rules are defined in Prometheus' configuration and are automatically evaluated.  Alerts Config alerting_rules.yml: groups: - name: blackbox_alerts rules: - alert: CertExpiration expr: ((probe_ssl_earliest_cert_expiry{job=&quot;blackbox&quot;} - time()) / 3600 / 24 &lt; 30) for: 90s labels: severity: 'critical' annotations: description: &quot;A certificate ({{ $labels.instance }}) is about to expire in 30 days!&quot; summary: &quot;A certificate ({{ $labels.instance }}) is about to expire in 30 days!&quot;   Key Fields in Prometheus Alert Configuration groups: Organizes alerts into logical groups. Alerts within the same group are evaluated together.alert: The name of the alert, which should describe the issue it triggers.expr: The PromQL expression that defines when the alert should be triggered. If the expression evaluates as true, the alert fires.for: The amount of time the condition must be met before triggering the alert.labels: Additional labels that provide context or are used for routing in Alertmanager.annotations: Human-readable descriptions or summaries of the alert.  To add, update, or remove an alert, you need to create a pull request in the repositoryhere. After that, please notify the infra team for a review and upgrade. ","version":"Next","tagName":"h2"},{"title":"error-express-handler","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/error-express-handler","content":"","keywords":"","version":"Next"},{"title":"Installâ€‹","type":1,"pageTitle":"error-express-handler","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/error-express-handler#install","content":" npm install --save @map-colonies/error-express-handler   ","version":"Next","tagName":"h2"},{"title":"Usageâ€‹","type":1,"pageTitle":"error-express-handler","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/error-express-handler#usage","content":" import express from 'express'; import { getErrorHandlerMiddleware } from '@map-colonies/error-express-handler'; process.env.NODE_ENV = 'development'; const app = express(); app.use('/meow', fn); app.use(getErrorHandlerMiddleware()); app.listen(8080, function() { console.log('server is up'); });   ","version":"Next","tagName":"h2"},{"title":"Flowâ€‹","type":1,"pageTitle":"error-express-handler","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/error-express-handler#flow","content":"   note This page was generated from a remote source. you can find it on https://github.com/MapColonies/error-express-handler/blob/master/README.md ","version":"Next","tagName":"h2"},{"title":"Grafana Tempo","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/Tempo","content":"","keywords":"","version":"Next"},{"title":"Example: Visualizing Tracesâ€‹","type":1,"pageTitle":"Grafana Tempo","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/monitoring/Tempo#example-visualizing-traces","content":" Once the OpenTelemetry Collector sends traces to Tempo, you can view and analyze these traces in Grafana.   ","version":"Next","tagName":"h2"},{"title":"JSON Schema Tips","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/json-schema/json-schema-tips","content":"","keywords":"","version":"Next"},{"title":"Map Colonies ESLint configs","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/eslint-config","content":"","keywords":"","version":"Next"},{"title":"Available Configsâ€‹","type":1,"pageTitle":"Map Colonies ESLint configs","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/eslint-config#available-configs","content":" ts-base: base configurations for typescript.react: rules for react (extends react-app).jest: rules for jest.  ","version":"Next","tagName":"h2"},{"title":"Installationâ€‹","type":1,"pageTitle":"Map Colonies ESLint configs","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/eslint-config#installation","content":" $ npm install --save-dev eslint @map-colonies/eslint-config   or  $ yarn add --dev eslint @map-colonies/eslint-config   ","version":"Next","tagName":"h2"},{"title":"Usageâ€‹","type":1,"pageTitle":"Map Colonies ESLint configs","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/eslint-config#usage","content":" Add the configs you want to the extend section of your eslintConfig of your package.json, or to your .eslintrc configuration file. Note: make sure to add ts-base last.   &quot;eslintConfig&quot;: { &quot;extends&quot;: [ &quot;@map-colonies/eslint-config/react&quot;, &quot;@map-colonies/eslint-config/ts-base&quot; ] }   Then add the path to your TypeScript configuration file to the parserOptions  &quot;eslintConfig&quot;: { &quot;parserOptions&quot;: { &quot;project&quot;: &quot;./path/to/your/tsconfig.json&quot; } }   ","version":"Next","tagName":"h2"},{"title":"Adding new Configsâ€‹","type":1,"pageTitle":"Map Colonies ESLint configs","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/eslint-config#adding-new-configs","content":" Add a new file and name it as you would like. Inside export the ESLint configuration.  module.exports = { extends: ['plugin:jest/recommended', 'plugin:jest/style'], plugins: ['jest'], env: { 'jest/globals': true, }, };   after you finished developing the config, make sure it works by using the --print-config flag of ESLint, in the project you use for testing.  $ npx eslint --print-config index.ts   Dont forget adding the config to this readme ðŸ˜Š  ","version":"Next","tagName":"h2"},{"title":"Issuesâ€‹","type":1,"pageTitle":"Map Colonies ESLint configs","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/eslint-config#issues","content":" If any linting error is appearing twice, or you have any other problem, please open an issue with all the details you have.  note This page was generated from a remote source. you can find it on https://github.com/MapColonies/eslint-config/blob/master/README.md ","version":"Next","tagName":"h2"},{"title":"Metadataâ€‹","type":1,"pageTitle":"JSON Schema Tips","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/json-schema/json-schema-tips#metadata","content":" You should use the title and description keywords to provide a description of the schema. This will help users understand what the schema is for.  { &quot;title&quot;: &quot;My Schema&quot;, &quot;description&quot;: &quot;This is a schema for my app&quot; }   ","version":"Next","tagName":"h2"},{"title":"Examplesâ€‹","type":1,"pageTitle":"JSON Schema Tips","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/json-schema/json-schema-tips#examples","content":" You can provide examples of how the schema should be used by using the examples keyword.  { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;name&quot;: { &quot;type&quot;: &quot;string&quot; } }, &quot;examples&quot;: { &quot;name&quot;: &quot;John Doe&quot; } }   ","version":"Next","tagName":"h2"},{"title":"Commentsâ€‹","type":1,"pageTitle":"JSON Schema Tips","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/json-schema/json-schema-tips#comments","content":" You can use the $comment keyword to provide comments in the schema. This can be useful for providing additional information about the schema.  { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;name&quot;: { &quot;type&quot;: &quot;string&quot; } }, &quot;$comment&quot;: &quot;This is a schema for my app&quot; }   For more information check the relevant JSON Schema Docs  ","version":"Next","tagName":"h2"},{"title":"Defaultâ€‹","type":1,"pageTitle":"JSON Schema Tips","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/json-schema/json-schema-tips#default","content":" When you have a default value for a property, you can use the default keyword to set it. This is useful for when you want to provide a default value for a property that is not required.  { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;name&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;default&quot;: &quot;John Doe&quot; } } }   The value will be provided by the config even if it is not set.  ","version":"Next","tagName":"h2"},{"title":"Requiredâ€‹","type":1,"pageTitle":"JSON Schema Tips","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/json-schema/json-schema-tips#required","content":" You can use the required keyword to specify which properties are required in the schema. By default, all properties are optional.  { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;name&quot;: { &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [&quot;name&quot;] }   The types generated for the schema will reflect the required properties.  type MySchema = { name: string; };   ","version":"Next","tagName":"h2"},{"title":"Enumsâ€‹","type":1,"pageTitle":"JSON Schema Tips","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/json-schema/json-schema-tips#enums","content":" You can use the enum keyword to specify a list of possible values for a property. It is not required to specify the type of the property when using enum, so its possible for the enum to be of different types.  { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;color&quot;: { &quot;enum&quot;: [&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;] } } }   The types generated will be a union of the enum values.  type MySchema = { color: &quot;red&quot; | &quot;green&quot; | &quot;blue&quot;; };   ","version":"Next","tagName":"h2"},{"title":"Refsâ€‹","type":1,"pageTitle":"JSON Schema Tips","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/json-schema/json-schema-tips#refs","content":" You can use the $ref keyword to reference another schema. The references can be either internal or external.  Reference can be used to reuse schemas and keep the schema definitions clean. Even for small schemas like Id, it is a good practice to use refs.  ","version":"Next","tagName":"h2"},{"title":"Internal Refsâ€‹","type":1,"pageTitle":"JSON Schema Tips","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/json-schema/json-schema-tips#internal-refs","content":" { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;name&quot;: { &quot;$ref&quot;: &quot;#/definitions/name&quot; } }, &quot;definitions&quot;: { &quot;name&quot;: { &quot;type&quot;: &quot;string&quot; } } }   ","version":"Next","tagName":"h3"},{"title":"External Refsâ€‹","type":1,"pageTitle":"JSON Schema Tips","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/json-schema/json-schema-tips#external-refs","content":" External refs are only valid to other schemas that are defined in the schemas repository. The way to reference is to use the ID of required the schema.  { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;name&quot;: { &quot;$ref&quot;: &quot;https://mapcolonies.com/common/schema/v1&quot; } } }   ","version":"Next","tagName":"h3"},{"title":"Definitionsâ€‹","type":1,"pageTitle":"JSON Schema Tips","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/json-schema/json-schema-tips#definitions","content":" You can use the definitions keyword to define reusable schemas. The definitions are not part of the validated schema, unless they are referenced.  { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;name&quot;: { &quot;$ref&quot;: &quot;#/definitions/name&quot; } }, &quot;definitions&quot;: { &quot;name&quot;: { &quot;type&quot;: &quot;string&quot; } } }   ","version":"Next","tagName":"h2"},{"title":"Schema Compositionâ€‹","type":1,"pageTitle":"JSON Schema Tips","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/json-schema/json-schema-tips#schema-composition","content":" In many cases you will want to compose multiple schemas together. JSON Schema provides a few keywords to help with this. This way you can reuse schemas and keep the schema definitions clean.  The classic example in our case is to extends the boilerplate schema to fit our service.  For more information check the relevant JSON Schema Docs  ","version":"Next","tagName":"h2"},{"title":"AllOfâ€‹","type":1,"pageTitle":"JSON Schema Tips","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/json-schema/json-schema-tips#allof","content":" The allOf keyword is used to combine multiple schemas together. The properties of the schemas are merged together.  The following schema  { &quot;allOf&quot;: [ { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;name&quot;: { &quot;type&quot;: &quot;string&quot; } } }, { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;age&quot;: { &quot;type&quot;: &quot;number&quot; } } } ] }   Will be equivalent to the following schema  { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;name&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;age&quot;: { &quot;type&quot;: &quot;number&quot; } } }   important When using additionalProperties: false in the schemas, the allOf keyword will not merge the properties together. Instead, it will require that all properties are present in the schema, Which is not possible. Check the JSON Schema Docs for more information.  ","version":"Next","tagName":"h3"},{"title":"OneOfâ€‹","type":1,"pageTitle":"JSON Schema Tips","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/json-schema/json-schema-tips#oneof","content":" The oneOf keyword is used to specify that only one of the schemas should be valid.  The following schema  { &quot;oneOf&quot;: [ { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;name&quot;: { &quot;type&quot;: &quot;string&quot; } } }, { &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;age&quot;: { &quot;type&quot;: &quot;number&quot; } } } ] }   If both properties are provided, the schema will be invalid.  ","version":"Next","tagName":"h3"},{"title":"Stringsâ€‹","type":1,"pageTitle":"JSON Schema Tips","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/json-schema/json-schema-tips#strings","content":" ","version":"Next","tagName":"h2"},{"title":"Formatsâ€‹","type":1,"pageTitle":"JSON Schema Tips","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/json-schema/json-schema-tips#formats","content":" It is recommended to use the format keyword to specify the format of the string. This will help users understand what the string is for and validate accordingly.  { &quot;type&quot;: &quot;string&quot;, &quot;format&quot;: &quot;email&quot; }   For a full list of formats check the ajv-formats repository.  ","version":"Next","tagName":"h3"},{"title":"Patternsâ€‹","type":1,"pageTitle":"JSON Schema Tips","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/json-schema/json-schema-tips#patterns","content":" You can use the pattern keyword to specify a regular expression that the string should match.  { &quot;type&quot;: &quot;string&quot;, &quot;pattern&quot;: &quot;^[a-zA-Z]+$&quot; }   ","version":"Next","tagName":"h3"},{"title":"Constâ€‹","type":1,"pageTitle":"JSON Schema Tips","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/json-schema/json-schema-tips#const","content":" You can use the const keyword to specify a constant value for the string.  { &quot;type&quot;: &quot;string&quot;, &quot;const&quot;: &quot;John Doe&quot; }  ","version":"Next","tagName":"h3"},{"title":"express-access-log-middleware","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/express-access-log-middleware","content":"","keywords":"","version":"Next"},{"title":"Usageâ€‹","type":1,"pageTitle":"express-access-log-middleware","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/express-access-log-middleware#usage","content":" import * as express from 'express'; import jsLogger from '@map-colonies/js-logger'; import httpLogger from '@map-colonies/express-access-log-middleware'; const app = express() const logger = jsLogger(); app.use(jsLogger({logger})); app.get('/', (req,res) =&gt; { res.json(hello: 'world'); }); app.listen(8080);   for more detailed usage check the pino-http documentation.  ","version":"Next","tagName":"h2"},{"title":"Configurationâ€‹","type":1,"pageTitle":"express-access-log-middleware","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/express-access-log-middleware#configuration","content":" name\ttype\tdefault value\tdescriptionlogger\tLogger The logger instance to use ignorePaths\tstring[]\tundefined\tThe paths to ignore logging customLogLevel\t(res, err) =&gt; log_level\tinfo for all under 500 status\tA function to set the log level of a request customSuccessMessage\t(res: ServerResponse) =&gt; string\tundefined\tfunction to set the success message customSuccessObject\t(req: IncomingMessage, res: ServerResponse, val: any) =&gt; object\tundefined\tfunction to set the success object customErrorMessage\t(error: Error, res: ServerResponse) =&gt; string\tundefined\tfunction to set the error message customErrorObject\t(req: IncomingMessage, res: ServerResponse, error: Error) =&gt; object\tundefined\tfunction to set the error object  note This page was generated from a remote source. you can find it on https://github.com/MapColonies/express-access-log-middleware/blob/master/README.md ","version":"Next","tagName":"h2"},{"title":"js logger","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/js-logger","content":"","keywords":"","version":"Next"},{"title":"Usageâ€‹","type":1,"pageTitle":"js logger","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/js-logger#usage","content":" import jsLogger from '@map-colonies/js-logger'; const logger = jsLogger(); logger.info('hello world'); logger.error({hello: 'world'});   for more detailed usage check the pino documentation.  ","version":"Next","tagName":"h2"},{"title":"Configurationâ€‹","type":1,"pageTitle":"js logger","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/js-logger#configuration","content":" name\ttype\tdefault value\tdescriptionenabled\tboolean\ttrue\tenables logging level\tstring\t'info'\tone of the supported level or silent to disable logging prettyPrint\tboolean\tfalse\tpretty print for developing purposes redact\tarray\tundefined\tarray of paths in object to be redacted from the log destination\tnumber / string\t1\tThe stream to send the log to, or file base\tobject\t{pid: process.pid, hostname: os.hostname}\tKey-value object added as child logger to each log line pinoCaller\tboolean\tfalse\tadds the call site of each log message to the log output  note This page was generated from a remote source. you can find it on https://github.com/MapColonies/js-logger/blob/master/README.md ","version":"Next","tagName":"h2"},{"title":"ts-npm-package-boilerplate","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/openapi-express-viewer","content":"ts-npm-package-boilerplate After cloning this template, please do the following: insert secrets to repo secrets for the github actions.replace every string &quot;ts-npm-package-boilerplate&quot; with your package name. note This page was generated from a remote source. you can find it on https://github.com/MapColonies/openapi-express-viewer/blob/master/README.md","keywords":"","version":"Next"},{"title":"prettier-config","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/prettier-config","content":"prettier-config After cloning this template, please do the following: insert secrets to repo secrets for the github actions.replace every string &quot;prettier-config&quot; with your package name. note This page was generated from a remote source. you can find it on https://github.com/MapColonies/prettier-config/blob/master/README.md","keywords":"","version":"Next"},{"title":"openapi-helpers","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/openapi-helpers","content":"","keywords":"","version":"Next"},{"title":"Installationâ€‹","type":1,"pageTitle":"openapi-helpers","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/openapi-helpers#installation","content":" Run the following commands:  npm install --save-dev @map-colonies/openapi-helpers supertest prettier openapi-typescript @types/express   ","version":"Next","tagName":"h2"},{"title":"types-generatorâ€‹","type":1,"pageTitle":"openapi-helpers","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/openapi-helpers#types-generator","content":" The package contains a script that wraps the openapi-typescript package and generates types for the openapi schema. The script also formats the generated types using prettier.  The command structure is as follows:  npx @map-colonies/openapi-helpers &lt;input-file&gt; &lt;output-file&gt; --format --add-typed-request-handler   For example:  npx @map-colonies/openapi-helpers ./openapi3.yaml ./src/openapi.d.ts --format --add-typed-request-handler   ","version":"Next","tagName":"h2"},{"title":"optionsâ€‹","type":1,"pageTitle":"openapi-helpers","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/openapi-helpers#options","content":" --format - format the generated types using prettier.--add-typed-request-handler - add the TypedRequestHandler type to the generated types.  ","version":"Next","tagName":"h3"},{"title":"TypedRequestHandlerâ€‹","type":1,"pageTitle":"openapi-helpers","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/openapi-helpers#typedrequesthandler","content":" The package contains a wrapper for the express types package that provides autocomplete for all the request Handlers to the API based on the openapi. The TypedRequestHandler is initialized with the the typed generated by openapi-typescript, and is configured based on operation name or method and path.  ","version":"Next","tagName":"h2"},{"title":"Usageâ€‹","type":1,"pageTitle":"openapi-helpers","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/openapi-helpers#usage","content":" import { TypedRequestHandlers } from '@map-colonies/openapi-helpers/typedRequestHandler'; import type { paths, operations } from './src/openapi.d.ts'; // Initialize the TypedRequestHandlers with the paths and operations types // This can be done in a separate file and exported, in the same file or even in the same line type MyHandlers = TypedRequestHandlers&lt;paths, operations&gt;; export class Controller { // Define the handler for the operation based method and path public getResource: MyHandlers['GET /resource'] = (req, res) =&gt; { res.status(httpStatus.OK).json({id: 1, description: 'description', name: 'name'}); }; // Define the handler for the operation based on the operation name public getResource: MyHandlers['getResource'] = (req, res) =&gt; { res.status(httpStatus.OK).json({id: 1, description: 'description', name: 'name'}); }; }   ","version":"Next","tagName":"h3"},{"title":"RequestSenderâ€‹","type":1,"pageTitle":"openapi-helpers","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/openapi-helpers#requestsender","content":" The package contains a wrapper for the supertest package that provides autocomplete for all the requests to the API based on the openapi. The requestSender is initialized with the server's base url and the openapi schema and the types exported by openapi-typescript.  import { RequestSender } from '@map-colonies/openapi-helpers/requestSender'; import type { paths, operations } from './src/openapi.d.ts'; const requestSender = await createRequestSender&lt;paths, operations&gt;('path/to/openapi3.yaml', expressApp);   The requestSender object contains all the paths and operations defined in the openapi schema. For example, to send a request to the getUsers operation with the /users path and with the GET method, you can use the following code:  const response = await requestSender.getUsers(); // or const response = await requestSender.sendRequest({ method: 'get', path: '/simple-request' });   The package supports all the operations defined in the openapi schema, either by operation name, or by using the sendRequest function with the method, path and parameters.  [!IMPORTANT] For the package function properly, you need to make sure that the following values are configured in your tsconfig.json or jsconfig.json files under compilerOptions: module: &quot;NodeNext&quot;moduleResolution: &quot;NodeNext&quot;  note This page was generated from a remote source. you can find it on https://github.com/MapColonies/openapi-helpers/blob/master/README.md ","version":"Next","tagName":"h2"},{"title":"read-pkg","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/read-pkg","content":"read-pkg After cloning this template, please do the following: insert secrets to repo secrets for the github actions.replace every string &quot;read-pkg&quot; with your package name. note This page was generated from a remote source. you can find it on https://github.com/MapColonies/read-pkg/blob/master/README.md","keywords":"","version":"Next"},{"title":"tsconfig","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/tsconfig","content":"","keywords":"","version":"Next"},{"title":"Installâ€‹","type":1,"pageTitle":"tsconfig","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/tsconfig#install","content":" npm install --save-dev @map-colonies/tsconfig   The config requires TypeScript 5.5 or later.  ","version":"Next","tagName":"h2"},{"title":"Usageâ€‹","type":1,"pageTitle":"tsconfig","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/tsconfig#usage","content":" tsconfig.json  { &quot;extends&quot;: &quot;@map-colonies/tsconfig/&lt;wanted tsconfig file&gt;&quot; }   ","version":"Next","tagName":"h2"},{"title":"Available filesâ€‹","type":1,"pageTitle":"tsconfig","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/tsconfig#available-files","content":" tsconfig-app.json - For use in services like ts-server-boilerplatetsconfig-library.json - For use in libraries published top npmtsconfig-base.json - The base tsconfig file  note This page was generated from a remote source. you can find it on https://github.com/MapColonies/tsconfig/blob/master/README.md ","version":"Next","tagName":"h2"},{"title":"Telemetry","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/telemetry","content":"","keywords":"","version":"Next"},{"title":"Motiveâ€‹","type":1,"pageTitle":"Telemetry","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/telemetry#motive","content":" This package goal is to make the experience of configuring and working with OpenTelemetry easier.  ","version":"Next","tagName":"h2"},{"title":"Manual for easy local grafana deploymentâ€‹","type":1,"pageTitle":"Telemetry","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/telemetry#manual-for-easy-local-grafana-deployment","content":" ","version":"Next","tagName":"h2"},{"title":"exampleâ€‹","type":1,"pageTitle":"Telemetry","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/telemetry#example","content":" Below are short examples for tracing and metrics. More examples are available at the examples folder, and the various opentelemetry repos.  ","version":"Next","tagName":"h2"},{"title":"Tracingâ€‹","type":1,"pageTitle":"Telemetry","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/telemetry#tracing","content":" The following code shows a simple example of how to work with tracing. please notice that you need to manually install any auto-instrumentation library that you require.  import { Tracing } from '@map-colonies/telemetry'; import { trace } from '@opentelemetry/api'; const tracing = new Tracing(); tracing.start(); const tracer = trace.getTracer('tracing-name') const span = tracer.startSpan('some-action'); span.setAttribute('some-attribute'); // DO STUFF span.end(); tracing.stop().then(() =&gt; console.log('done'));   Another way for initialize tracing with custom resource:  import { Tracing } from '@map-colonies/telemetry'; import { Resource } from '@opentelemetry/resources'; const resource = new Resource({ 'service.version': number, 'service.name': 'my-service-name' }); const tracing = new Tracing([], resource); ...   ","version":"Next","tagName":"h3"},{"title":"Metricsâ€‹","type":1,"pageTitle":"Telemetry","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/telemetry#metrics","content":" The following code shows a simple example of how to work with metrics.  import { Metrics } from '@map-colonies/telemetry'; const metrics = new Metrics('sample-meter'); const meter = metrics.start(); const counter = meter.createCounter('sample_counter'); counter.add(1); metrics.stop().then(() =&gt; console.log('done'));   ","version":"Next","tagName":"h3"},{"title":"Metrics middlewareâ€‹","type":1,"pageTitle":"Telemetry","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/telemetry#metrics-middleware","content":" The package provides a middleware for express that will automatically measure the duration of each request and the number of requests. In addition the middleware can be configured to collect NodeJS metrics.  import { collectMetricsExpressMiddleware } from '@map-colonies/telemetry/prom-metrics'; import express from 'express'; import { Registry } from 'prom-client'; const prom = collectMetricsExpressMiddleware({ registry: new Registry(), labels: { meow: 'a' } }); app.use('/metrics', prom); app.get('/', (req, res) =&gt; { res.json({ x: 'd' }); }); app.listen(8080, () =&gt; console.log('server listening on 8080'));   [!NOTE] If you are not running the express-openapi-validator middleware, its recommended to turn off the includeOperationId option in the collectMetricsExpressMiddleware function as the operation label will always be null.  ","version":"Next","tagName":"h2"},{"title":"Semantic Conventionsâ€‹","type":1,"pageTitle":"Telemetry","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/telemetry#semantic-conventions","content":" The package's Semantic Conventions submodule defines a common set of (semantic) attributes which provide meaning to data when collecting, producing and consuming it.â€‹  Based on the official OpenTelemetry conventions  Link to full documentation  ","version":"Next","tagName":"h2"},{"title":"Configurationâ€‹","type":1,"pageTitle":"Telemetry","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/telemetry#configuration","content":" ","version":"Next","tagName":"h2"},{"title":"Common configurationâ€‹","type":1,"pageTitle":"Telemetry","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/telemetry#common-configuration","content":" name\tallowed value\tdefault value\tdescriptionTELEMETRY_SERVICE_NAME\tstring\tfrom package.json\tThe service name TELEMETRY_SERVICE_VERSION\tstring\tfrom package.json\tThe service version TELEMETRY_HOST_NAME\tstring\tos.hostname()\tThe host name    ","version":"Next","tagName":"h3"},{"title":"Tracing configurationâ€‹","type":1,"pageTitle":"Telemetry","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/telemetry#tracing-configuration","content":" name\tallowed value\tdefault value\tdescriptionTELEMETRY_TRACING_ENABLED\t'true', 'false'\t'false'\tShould Tracing be enabled TELEMETRY_TRACING_URL*\tstring\thttp://localhost:4318/v1/traces\tThe URL to the OpenTelemetry Collector TELEMETRY_TRACING_RATIO\tfloat\t1\tThe amount of traces to sample  * required (only when tracing is enabled).   ","version":"Next","tagName":"h3"},{"title":"Metric configurationâ€‹","type":1,"pageTitle":"Telemetry","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/telemetry#metric-configuration","content":" name\tallowed value\tdefault value\tdescriptionTELEMETRY_METRICS_ENABLED\t'true', 'false'\t'false'\tShould Metrics be enabled TELEMETRY_METRICS_URL*\tstring\thttp://localhost:4318/v1/metrics\tThe URL to the OpenTelemetry Collector TELEMETRY_METRICS_INTERVAL\tnumber\t15000\tThe interval in miliseconds between sending data to the collector  * required (only when tracing is enabled).  ","version":"Next","tagName":"h3"},{"title":"How to releaseâ€‹","type":1,"pageTitle":"Telemetry","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/telemetry#how-to-release","content":" Run the command npm run release -- to bump the version in all the files and create a changelog.  For more detailed documentation and examples check: https://github.com/conventional-changelog/standard-version  note This page was generated from a remote source. you can find it on https://github.com/MapColonies/telemetry/blob/master/README.md ","version":"Next","tagName":"h3"},{"title":"Config","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config","content":"","keywords":"","version":"Next"},{"title":"API Documentationâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#api-documentation","content":" This section describes the API provided by the package for interacting with the configuration.  ","version":"Next","tagName":"h2"},{"title":"ConfigInstance<T>â€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#configinstancet","content":" The ConfigInstance interface represents the your way to interact with the configuration. It provides methods to retrieve configuration values and parts.T is the typescript type associated with the chosen schema. it can be imported from the @map-colonies/schemas package.  Methodsâ€‹  get&lt;TPath extends string&gt;(path: TPath): _.GetFieldType&lt;T, TPath&gt;â€‹  Description: Retrieves the value at the specified path from the configuration object. Note that the type of returned object is based on the path in the schema.Parameters: path (TPath): The path to the desired value. Returns: The value at the specified path.  getAll(): Tâ€‹  Description: Retrieves the entire configuration object.Returns: The entire configuration object.  getConfigParts(): { localConfig: object; config: object; envConfig: object }â€‹  Description: Retrieves different parts of the configuration object before being merged and validated. Useful for debugging.Returns: An object containing the localConfig, config, and envConfig parts of the configuration. localConfig: The local configuration object.config: The remote configuration object.envConfig: The environment configuration object.  getResolvedOptions(): BaseOptionsâ€‹  Description: Retrieves the resolved options from the configuration object. Useful for debugging.Returns: The resolved options, which are an instance of BaseOptions.  initializeMetrics(registry: promClient.Registry): voidâ€‹  Description: Initializes the metrics for the configuration.Parameters: registry (promClient.Registry): The prometheus registry to use for the metrics.  Configuration Options  This package allows you to configure various options for loading and managing configurations. Below are the available options and their descriptions.  ","version":"Next","tagName":"h3"},{"title":"Optionsâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#options","content":" ","version":"Next","tagName":"h2"},{"title":"schemaâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#schema","content":" Type: T extends SchemaWithTypeOptional: falseDescription: The schema of the configuration object.  ","version":"Next","tagName":"h3"},{"title":"configNameâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#configname","content":" Type: stringOptional: trueDescription: The name of the remote configuration.Environment Variable: CONFIG_NAME  ","version":"Next","tagName":"h3"},{"title":"versionâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#version","content":" Type: 'latest' | numberOptional: trueDefault: latestDescription: The version of the remote configuration. It can be either 'latest' or a number.Environment Variable: CONFIG_VERSION  ","version":"Next","tagName":"h3"},{"title":"configServerUrlâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#configserverurl","content":" Type: stringOptional: trueDefault: http://localhost:8080Description: The URL of the configuration server.Environment Variable: CONFIG_SERVER_URL  ","version":"Next","tagName":"h3"},{"title":"offlineModeâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#offlinemode","content":" Type: booleanOptional: trueDefault: falseDescription: Indicates whether the configuration should be loaded in offline mode.Environment Variable: CONFIG_OFFLINE_MODE  ","version":"Next","tagName":"h3"},{"title":"ignoreServerIsOlderVersionErrorâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#ignoreserverisolderversionerror","content":" Type: booleanOptional: trueDescription: Indicates whether to ignore the error when the server version is older than the requested version.Environment Variable: CONFIG_IGNORE_SERVER_IS_OLDER_VERSION_ERROR  ","version":"Next","tagName":"h3"},{"title":"localConfigPathâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#localconfigpath","content":" Type: stringOptional: trueDefault: ./configDescription: The path to the local configuration folder.  ","version":"Next","tagName":"h3"},{"title":"Environment Variable Configurationâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#environment-variable-configuration","content":" The following environment variables can be used to configure the options:  CONFIG_NAME: Sets the configName option.CONFIG_VERSION: Sets the version option.CONFIG_SERVER_URL: Sets the configServerUrl option.CONFIG_OFFLINE_MODE: Sets the offlineMode option.CONFIG_IGNORE_SERVER_IS_OLDER_VERSION_ERROR: Sets the ignoreServerIsOlderVersionError option.  ","version":"Next","tagName":"h2"},{"title":"Configuration Merging and Validationâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#configuration-merging-and-validation","content":" The package supports merging configurations from multiple sources (local, remote, and environment variables) and then validates the merged configuration against the schema.  ","version":"Next","tagName":"h2"},{"title":"Local Configurationâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#local-configuration","content":" The local configuration is loaded from the path specified by the localConfigPath option. The default path is ./config.  ","version":"Next","tagName":"h3"},{"title":"Remote Configurationâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#remote-configuration","content":" The remote configuration is fetched from the server specified by the configServerUrl option.If the version is set to 'latest', the latest version of the configuration is fetched. Otherwise, the specified version is fetched.  ","version":"Next","tagName":"h3"},{"title":"Environment Variablesâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#environment-variables","content":" Configuration options can be overridden by setting the corresponding environment variables as described in schema using the x-env-value key.  ","version":"Next","tagName":"h3"},{"title":"Merging Configurationsâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#merging-configurations","content":" The configurations are merged in the following order of precedence: Environment variablesRemote configurationLocal configuration If a configuration option is specified in multiple sources, the value from the source with higher precedence (as listed above) is used.  ","version":"Next","tagName":"h3"},{"title":"Validationâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#validation","content":" After merging, the final configuration is validated against the defined schema using ajv.The validation ensures that all required properties are present, and the types and values of properties conform to the schema.Any default value according to the schema is added to the final object.If the validation fails, an error is thrown, indicating the invalid properties and their issues.  Error handling  This section describes the possible errors that can occur when using the package, along with their codes and payload structures.  ","version":"Next","tagName":"h3"},{"title":"Identifying errorsâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#identifying-errors","content":" The package exposes a helper function called isConfigError to assert what is the error that was thrown and handle it as needed.  import { config, isConfigError } from '@map-colonies/config'; try { const configInstance = await config({ configName: 'boiler-config', configServerUrl: 'http://localhost:8080', schema: commonBoilerplateV4, version: 'latest', offlineMode: false }); } catch (error) { if (isConfigError(error, 'configValidationError')) { console.error('Config validation error:', error.payload); } }   ","version":"Next","tagName":"h2"},{"title":"Errorsâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#errors","content":" ","version":"Next","tagName":"h2"},{"title":"optionValidationErrorâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#optionvalidationerror","content":" Code: 1Payload: ValidationError[]Description: This error occurs when there is a validation error with one of the configuration options.  ","version":"Next","tagName":"h3"},{"title":"configValidationErrorâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#configvalidationerror","content":" Code: 2Payload: ValidationError[]Description: This error occurs when the configuration as a whole fails validation.  ","version":"Next","tagName":"h3"},{"title":"httpResponseErrorâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#httpresponseerror","content":" Code: 3Payload: { headers: Record&lt;string, string&gt;; statusCode: number; body: string; } Description: This error occurs when an HTTP request results in an error response. The payload includes the response headers, status code and body.  ","version":"Next","tagName":"h3"},{"title":"httpGeneralErrorâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#httpgeneralerror","content":" Code: 4Payload: ErrorDescription: This error occurs when there is a general HTTP error. The payload contains the error object.  ","version":"Next","tagName":"h3"},{"title":"schemaNotFoundErrorâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#schemanotfounderror","content":" Code: 5Payload: { schemaPath: string; } Description: This error occurs when the specified schema cannot be found. The payload includes the path of the missing schema.  ","version":"Next","tagName":"h3"},{"title":"schemasPackageVersionMismatchErrorâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#schemaspackageversionmismatcherror","content":" Code: 6Payload: { remotePackageVersion: string; localPackageVersion: string; } Description: This error occurs when there is a version mismatch between the remote and local schema packages. The payload includes the versions of both the remote and local packages.  ","version":"Next","tagName":"h3"},{"title":"schemaVersionMismatchErrorâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#schemaversionmismatcherror","content":" Code: 7Payload: { remoteSchemaVersion: string; localSchemaVersion: string; } Description: This error occurs when there is a version mismatch between the remote and local schemas. The payload includes the versions of both the remote and local schemas.  ","version":"Next","tagName":"h3"},{"title":"promClientNotInstalledErrorâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#promclientnotinstallederror","content":" Code: 8Payload: ErrorDescription: This error occurs when the prom-client package is not installed. The payload contains the error object.  Debugging  If for some reason you want to debug the package you can either use the getConfigParts or the getResolvedOptions functions described in the API or use the more powerful debug logger.  The package debug logger is implemented using the debug npm package and is configured using the DEBUG Environment variable.  The following are the values you can configure to use the debug option.  ","version":"Next","tagName":"h3"},{"title":"DEBUG=*â€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#debug","content":" Enables all the logs. Note that setting this option might enable debug logging of other packages.  ","version":"Next","tagName":"h3"},{"title":"DEBUG=@map-colonies/config*â€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#debugmap-coloniesconfig","content":" Enables all the logs available in this package.  ","version":"Next","tagName":"h3"},{"title":"DEBUG=@map-colonies/config:configâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#debugmap-coloniesconfigconfig","content":" Enables only the logs related to the main logic of the package.  ","version":"Next","tagName":"h3"},{"title":"DEBUG=@map-colonies/config:envâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#debugmap-coloniesconfigenv","content":" Enables only the logs related to parsing environment variables from schemas, and retrieving them for use in the configuration.  ","version":"Next","tagName":"h3"},{"title":"DEBUG=@map-colonies/config:httpâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#debugmap-coloniesconfighttp","content":" Enables only the logs related to http requests to the config-server.  ","version":"Next","tagName":"h3"},{"title":"DEBUG=@map-colonies/config:optionsâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#debugmap-coloniesconfigoptions","content":" Enables only the logs related to parsing and validation of the package initialization options.  ","version":"Next","tagName":"h3"},{"title":"DEBUG=@map-colonies/config:schemasâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#debugmap-coloniesconfigschemas","content":" Enables only the logs related to the retrieving of schemas.  ","version":"Next","tagName":"h3"},{"title":"DEBUG=@map-colonies/config:validatorâ€‹","type":1,"pageTitle":"Config","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/packages/config#debugmap-coloniesconfigvalidator","content":" Enables only the logs related to the validation of configurations.  note This page was generated from a remote source. you can find it on https://github.com/MapColonies/config/blob/master/README.md ","version":"Next","tagName":"h3"},{"title":"Release Management Guide","type":0,"sectionRef":"#","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/release-management","content":"","keywords":"","version":"Next"},{"title":"Libot Version Tagâ€‹","type":1,"pageTitle":"Release Management Guide","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/release-management#libot-version-tag","content":" The Libot Version is an alias to a specific helm-charts release version. This means that it represents the entirety of all service releases for the production deployment and it's structure.  ","version":"Next","tagName":"h2"},{"title":"Service Branching Strategyâ€‹","type":1,"pageTitle":"Release Management Guide","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/release-management#service-branching-strategy","content":" Each service must maintain at least one main branch to track the latest stable release and major version updates:  master: Represents the latest stable version of the service.Merges to this branch indicate the most recent production-ready release. MAJOR.x.x (e.g., 1.x.x, 2.x.x): Tracks the major version series of a service.A new branch is created for every major version that is smaller than the latest major version.Example: If a service's latest release version is 3.3.0 and a new bug fix needs to be merged to latest release with major version '2', a pull request (PR) will be created for both: master (represents 3.x.x) - only if the bug fix relevant to this release too2.x.x (to track the new version with the fix). Once merged, both branches will represent a release with the bug fix, while the 1.x.x branch remains unchanged.  ","version":"Next","tagName":"h2"},{"title":"Service Release Flowâ€‹","type":1,"pageTitle":"Release Management Guide","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/release-management#service-release-flow","content":"   Explanation:  The developer creates a PR On merging to master: release-please takes action and opens a PR release with these changes: Updating package.json to the new release versionUpdating CHANGELOG with the new versionUpdating helm chart version info The release version will be based on (SemVer) (according to the PR title) On merging the release PR to master, release-please will create a new release On new release, some automate workflows will be triggered: Push to NPM (for libraries)Build a docker image and push to registry (if a Dockerfile exists)Pack the helm chart and push to registry (if a helm chart exists)  End State:  New release version based on SemVermaster points to the new releaseNew release is pushed to npm, docker and helm registries  info The deployment part is described in the next section  ","version":"Next","tagName":"h2"},{"title":"Helming Strategyâ€‹","type":1,"pageTitle":"Release Management Guide","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/release-management#helming-strategy","content":" warning This strategy does not include the checks during development. It assumes that the new released service is properly checked and its updated chart is good. All the development tests should happen before the service was released so in this flow we assume that the chart is good and does not require deployment in order to know if it is a good chart    The project includes two different repositories that represent two different flows:  Chart Flow using &quot;Helm-Charts&quot; repo - Handling the new charts and versions from the services and creation of new &quot;Libot Version&quot; Values Flow using &quot;Helm-Values&quot; repo - Handling the deployment in openshift based on the new &quot;Libot Version&quot; that was created in helm-charts  ","version":"Next","tagName":"h2"},{"title":"Explanationâ€‹","type":1,"pageTitle":"Release Management Guide","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/release-management#explanation","content":" With this strategy, we separate the charts from the values.  helm-charts repo will contain only the chart, which is the deployment structure, and a default values file. The structure shouldn't change much from now on (for the current set of features). helm-values repo will contain only the values based on the relevant chartand the helm-chart version it's based on (a file named &quot;helm-chart.json&quot; in the repo). The value files that represent openshift namespaces should not be part of the structure (as they are only config files), they should be in a different repo. This gives us the following benefits:  Avoiding &quot;trash&quot; commits Imagine if we keep the values with the chart, Now if we want to change the log level from info to debug, we will have to commit the change in the repo itself and this shouldn't be. That way we will &quot;trash&quot; the repo with a lot with unnecessary commits that don't related to chart itself. Allowing informative versions of helm-charts If we keep the chart and the values together, We will have a hard time to handle properly the release of helm-chart duo to the many changes on the values that does not require a new release. From the other hand, there are changes in the values file that we want to change. Imagine that you want to change the default port to be 80. This is a &quot;structure&quot; change because you want to change a default value so it does require a new release. So you won't know what requires a new release and what is not. With the separated values and charts, you will have just default values in the chart so every change in that values file will require new release. One and only structureCurrently, the charts in the network can be different from the charts in Azure because the charts can be modified. Using the new strategy, we make sure that there will be one source of truth and it sits in helm-charts outside the internal network and only there! Simpler deployment IF we keep them together, we will need to manage every chart (along with the values) in different branches. That way we will have hard time to manage the charts in all the branches. With the new strategy, we need to &quot;worry&quot; only the values and the version of the chart when we deploy. We won't manage the chart anymore. We can only run this command template: helm install myrelease oci://HELM_REGISTRY/helm-charts/raster-ingestion --version 3.1.0 And that's it! We don't need the chart in the repo anymore. We make the structure and the config independent to each other.  Let's walk through the flows!  ","version":"Next","tagName":"h3"},{"title":"Chart Flowâ€‹","type":1,"pageTitle":"Release Management Guide","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/release-management#chart-flow","content":"   This flow suppose to handle the update of new versions of services and creation of new &quot;Libot Release&quot;.  When a service has a new release and helm folder (points that its chart in helm-charts needs to be updated), open a PR in helm-charts that update the version of the service.  note The developer should see the PR and make more changes if necessary  After the PR is ready, merge the PR.  info The PR can contain multiple commits so if there are more than one service that triggered the PR (more than one chart that needs to be updated), the PR will contain all the changes so the merge of the PR will update the versions of all the relevant services in helm-charts. tip If there is a service that you don't want to include it yet in helm-charts, you can disable the workflow that triggers that PR  On &quot;master&quot; push: Create a new &quot;Libot Version&quot; using release-please. The release-please should also update the docker and helm versions of the services in: artifacts.json file inside the domain that represent the information of all services and versions of the domainlibot-artifacts.json in the root directory that represent the information of all services and versions of all domains Package all the charts and push them to helm registry. For example: On new libot release with version 3.0.1, we will push to helm registry the raster-ingestion chart with this name: rater-ingestion-3.0.1.  End State:  New &quot;Libot Version&quot;Charts of the new version packed in helm registry.  Now, we have an updated structure of the chart and we can move on to the values.  ","version":"Next","tagName":"h3"},{"title":"Values Flowâ€‹","type":1,"pageTitle":"Release Management Guide","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/release-management#values-flow","content":"   This flow suppose to handle the update of the namespaces values (integration and prod) and the deployment in openshift.  On new release, there will be a PR on stage branch in helm-values repo that updates the &quot;Libot Version&quot; in libot-version.json file There will be an auto deploy in &quot;integration&quot; namespace After testing it and make sure the values are OK, the developer will merge the PR Once the stage branch is updated, a new PR will be automatically created in &quot;prod&quot; branch to update the &quot;Libot Version&quot; in the file The developer will manually deploy using the prod values file After testing it and make sure the values are OK, the developer will merge the PR to &quot;prod&quot; branch  End State:  Updated values in all branches based on &quot;Libot Version&quot;Updated libot-version.json file in every branchUpdated deployments in all namespaces in openshift  info The values flow is the same flow in the network. There will be helm-values repo based on branches (each branch will represent a namespace in openshift) and in every branch there will be a file named: libot-version.json that will represent the related chart version  ","version":"Next","tagName":"h3"},{"title":"Bundle Strategyâ€‹","type":1,"pageTitle":"Release Management Guide","url":"/infra-portal/pr-preview/pr-26/docs/knowledge-base/release-management#bundle-strategy","content":"   You can see more about bundler and unbundler here. The only change is that the bundler pipeline will be able to create a bundle based on the &quot;Libot Version&quot;. ","version":"Next","tagName":"h2"}],"options":{"id":"default"}}